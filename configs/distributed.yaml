# Distributed AlphaZero Training Configuration
# =============================================
#
# ARCHITECTURE (Redis-based, no Ray):
# - Head node: Redis, Coordinator, Training worker, Grafana, Prometheus, MLflow
# - Remote workers: Standalone Python processes connecting via Redis
#
# CONFIG HIERARCHY:
# 1. Global settings (MCTS simulations, max_nodes) - same for all workers
# 2. Device-specific batch sizes - optimized per hardware type
# 3. Local overrides - via CLI flags (--batch-size) for unique machines

# =============================================================================
# GLOBAL TRAINING SETTINGS (applies to ALL workers)
# =============================================================================


game:
  batch_size: 16               # Base game batch size (overridden by device_configs)
  max_episode_steps: 500  # Max moves per game, safety to prevent infinite moves, todo; log metric: max_episode_limit_hit_count
  short_game: False # switched to full game after 3000+ games collected
  simple_doubles: False # simple doubles only plays doubles to limit randomness

mcts:
  collect_simulations: 200     # MCTS iterations per move (GLOBAL)    changed to collect specifically
  eval_simulations: 50         # added specifically for eval ,
  max_nodes: 2000               # Maximum tree size (GLOBAL)
  warm_tree_simulations: 500   # MCTS sims for warm tree (0 = disabled)
  warm_tree_max_nodes: 10000   # Max nodes for warm tree expansion
  persist_tree: true           # Keep tree between moves
  # Temperature schedule (Stochastic MuZero paper style)
  # Values: [1.0, 0.5, 0.1] for first [1e5, 2e5, 3e5] steps, then 0.0 (greedy)
  temperature_schedule_values: [0.5, 0.3, 0.1, 0.0]
  temperature_schedule_steps: [350000, 400000, 500000]
  #discount: -1.0               # -1 for two-player zero-sum games

training:
  batch_size: 128              # Base training batch size (overridden by device_configs)
  steps_per_epoch: 20000           # Training steps run once games_per_epoch new games are collected
  learning_rate: 0.0003            # Adam learning rate
  decode_threads: 8                # CPU threads for msgpack decode/stack (0=off)
  batch_reuse_steps: 4             # Run N updates per sampled batch to reduce CPU bottleneck
  # When training and GPU collection share the same GPU (head node), pause GPU collection
  # during training to avoid severe contention and keep training throughput stable.
  pause_collection_during_training: true
  # Bearoff value weight: multiplier for loss on bearoff positions (known-perfect values)
  # Higher values emphasize learning accurate endgame values. Set to 1.0 to disable.
  bearoff_enabled: True           # disable bearoff DB completely
  bearoff_table_path: "./data/bearoff_15_v4"  # path to bearoff table (without extension, adds .bin and .json)
  bearoff_value_weight: 2.0        # the learning rate multiplier for values pulled from bearoff db
  lookup_enabled: False            # disables the lookup positions.
  lookup_learning_weight: 1.5      # this is for future if we have specific anchor positions with value AND policy
  l2_reg_lambda: 0.0001            # L2 regularization
  games_per_epoch: 600               # New games before training step (~30 min with 3 workers) 
  surprise_weight: 0.2             # Blend of uniform (0) vs surprise-weighted (1) sampling
  checkpoint_epoch_interval: 5     # number of epochs berfore we write checkpoint to disk
  max_checkpoints: 5

# =============================================================================
# GNU BACKGAMMON EVALUATION SETTINGS
# =============================================================================
# These settings control GNUBG evaluation strength for the gnubg eval type.
# Higher ply and more move filter candidates = stronger but slower evaluation.

gnubg:
  ply: 1                  # Search depth (0=fastest, 1=medium, 2=strongest)
  shortcuts: 0            # Evaluation shortcuts (0=disabled, 1=faster but weaker)
  osdb: 1                 # One-sided bearoff database (0=disabled, 1=enabled)
  #  what is this? move_filters: [8, 4, 2, 2]  # Candidates at each ply level

# =============================================================================
# INFRASTRUCTURE
# =============================================================================

redis:
  host: "localhost"
  db: 0
  # use head host host: "100.105.50.111"  # Head node (Tailscale IP)
  # use head host_local host_local: "192.168.20.40"  # LAN fallback
  port: 6379
  password: "bgai-password"
  buffer_capacity: 100000   # Max experiences
  episode_capacity: 5000    # Max episodes

head:
  host: "100.105.50.111"       # Head node IP (Tailscale)
  host_local: "192.168.20.40"  # LAN fallback

mlflow:
  # Use localhost since only the training worker on the head node needs MLFlow
  # This avoids DNS rebinding security issues with external IPs
  # removed, use head host or head host_localtracking_uri: "http://localhost:5000"
  port: 5000
  experiment_name: "bgai-training"
  artifact_location: "./mlruns"

coordinator:
  heartbeat_timeout: 30.0   # Seconds before worker considered dead
  heartbeat_interval: 10.0  # Seconds between heartbeats
  weight_push_interval: 10  # Push weights every N training steps

# =============================================================================
# NEURAL NETWORK ARCHITECTURE
# =============================================================================

network:
  hidden_dim: 256
  num_blocks: 6
  num_actions: 156          # Backgammon action space
  # does this incorporate the new output value dimension size? 


# =============================================================================
# DEVICE-SPECIFIC BATCH SIZES
# =============================================================================
# These are applied automatically based on detected hardware.
# Only batch sizes vary - MCTS settings remain global.
# Workers can override batch_size via --batch-size CLI flag.

device_configs:
  cuda:
    # NVIDIA GPUs (RTX 4090, etc.)
    # With mutually exclusive training/collection, each gets full GPU compute
    # Memory is split 45%/45% but compute alternates for 100% utilization
    # batch_size=128 with max_nodes=2000 fits in 11GB with ~62% GPU util
    # Keep this conservative so the head GPU game worker stays alive.
    game_batch_size: 64
    # Fixed per-process GPU memory caps (GB). Prefer these over fractions.
    # Keep sums below total VRAM so multiple JAX processes can coexist.
    game_gpu_memory_gb: 15.0
    train_batch_size: 4096
    train_gpu_memory_gb: 6.0
    eval_batch_size: 32
    eval_gpu_memory_gb: 2.0

  metal:
    # Apple Silicon (M1, M2, M3)
    game_batch_size: 24
    train_batch_size: 128
    eval_batch_size: 16

  tpu:
    # Google Cloud TPUs
    game_batch_size: 128
    train_batch_size: 512
    eval_batch_size: 64

  cpu:
    # CPU-only fallback
    game_batch_size: 24
    train_batch_size: 64
    eval_batch_size: 8

# =============================================================================
# PATHS
# =============================================================================

checkpoint_dir: "./checkpoints"
log_dir: "./logs"

# =============================================================================
# MONITORING
# =============================================================================

prometheus:
  port: 9090
  scrape_interval: 15

grafana:
  port: 3000
