# Distributed AlphaZero Training Configuration
# =============================================
#
# ARCHITECTURE (Redis-based, no Ray):
# - Head node: Redis, Coordinator, Training worker, Grafana, Prometheus, MLflow
# - Remote workers: Standalone Python processes connecting via Redis
#
# CONFIG HIERARCHY:
# 1. Global settings (MCTS simulations, max_nodes) - same for all workers
# 2. Device-specific batch sizes - optimized per hardware type
# 3. Local overrides - via CLI flags (--batch-size) for unique machines

# =============================================================================
# GLOBAL TRAINING SETTINGS (applies to ALL workers)
# =============================================================================


game:
  batch_size: 16               # Base game batch size (overridden by device_configs)
  max_episode_steps: 500  # Max moves per game, safety to prevent infinite moves, todo; log metric: max_episode_limit_hit_count
  short_game: True # starting position is a mid game position rather than complete beginning
  simple_doubles: False # simple doubles only plays doubles to limit randomness

mcts:
  collect_simulations: 200     # MCTS iterations per move (GLOBAL)    changed to collect specifically
  eval_simulations: 50         # added specifically for eval ,
  max_nodes: 2000               # Maximum tree size (GLOBAL)
  warm_tree_simulations: 500   # MCTS sims for warm tree (0 = disabled)
  warm_tree_max_nodes: 10000   # Max nodes for warm tree expansion
  persist_tree: true           # Keep tree between moves
  temperature_start: 0.8       # Initial exploration
  temperature_end: 0.2         # End temperature.        
  temperature_epochs:  50      # The number of epochs it takes to go from start to end, then we just stay at ending temp 
  discount: -1.0               # -1 for two-player zero-sum games

training:
  batch_size: 128              # Base training batch size (overridden by device_configs)
  steps_per_epoch: 20000           # Training steps run once games_per_epoch new games are collected
  learning_rate: 0.0003            # Adam learning rate
  # Bearoff value weight: multiplier for loss on bearoff positions (known-perfect values)
  # Higher values emphasize learning accurate endgame values. Set to 1.0 to disable.
  bearoff_enabled: True           # disable bearoff DB completely
  bearoff_value_weight: 2.0        # the learning rate multiplier for values pulled from bearoff db
  lookup_enabled: False            # disables the lookup positions.
  lookup_learning_weight: 1.5      # this is for future if we have specific anchor positions with value AND policy
  l2_reg_lambda: 0.0001            # L2 regularization
  games_per_epoch: 200               # New games before training step (lower = more frequent training) 
  surprise_weight: 0.2             # Blend of uniform (0) vs surprise-weighted (1) sampling
  checkpoint_epoch_interval: 5     # number of epochs berfore we write checkpoint to disk
  max_checkpoints: 5

# =============================================================================
# GNU BACKGAMMON EVALUATION SETTINGS
# =============================================================================
# These settings control GNUBG evaluation strength for the gnubg eval type.
# Higher ply and more move filter candidates = stronger but slower evaluation.

gnubg:
  ply: 1                  # Search depth (0=fastest, 1=medium, 2=strongest)
  shortcuts: 0            # Evaluation shortcuts (0=disabled, 1=faster but weaker)
  osdb: 1                 # One-sided bearoff database (0=disabled, 1=enabled)
  #  what is this? move_filters: [8, 4, 2, 2]  # Candidates at each ply level

# =============================================================================
# INFRASTRUCTURE
# =============================================================================

redis:
  host: "localhost"
  db: 0
  # use head host host: "100.105.50.111"  # Head node (Tailscale IP)
  # use head host_local host_local: "192.168.20.40"  # LAN fallback
  port: 6379
  password: "bgai-password"
  buffer_capacity: 100000   # Max experiences
  episode_capacity: 5000    # Max episodes

head:
  host: "100.105.50.111"       # Head node IP (Tailscale)
  host_local: "192.168.20.40"  # LAN fallback

mlflow:
  # Use localhost since only the training worker on the head node needs MLFlow
  # This avoids DNS rebinding security issues with external IPs
  # removed, use head host or head host_localtracking_uri: "http://localhost:5000"
  port: 5000
  experiment_name: "bgai-training"
  artifact_location: "./mlruns"

coordinator:
  heartbeat_timeout: 30.0   # Seconds before worker considered dead
  heartbeat_interval: 10.0  # Seconds between heartbeats
  weight_push_interval: 10  # Push weights every N training steps

# =============================================================================
# NEURAL NETWORK ARCHITECTURE
# =============================================================================

network:
  hidden_dim: 256
  num_blocks: 6
  num_actions: 156          # Backgammon action space
  # does this incorporate the new output value dimension size? 


# =============================================================================
# DEVICE-SPECIFIC BATCH SIZES
# =============================================================================
# These are applied automatically based on detected hardware.
# Only batch sizes vary - MCTS settings remain global.
# Workers can override batch_size via --batch-size CLI flag.

device_configs:
  cuda:
    # NVIDIA GPUs (RTX 4090, etc.)
    # With mutually exclusive training/collection, each gets full GPU compute
    # Memory is split 45%/45% but compute alternates for 100% utilization
    # batch_size=128 with max_nodes=2000 fits in 11GB with ~62% GPU util
    game_batch_size: 200
    game_gpu_memory_fraction: 0.3
    train_batch_size: 4096
    train_gpu_memory_fraction: 0.3
    eval_batch_size: 32
    eval_gpu_memory_fraction: 0.1

  metal:
    # Apple Silicon (M1, M2, M3)
    game_batch_size: 24
    train_batch_size: 128
    eval_batch_size: 16

  tpu:
    # Google Cloud TPUs
    game_batch_size: 128
    train_batch_size: 512
    eval_batch_size: 64

  cpu:
    # CPU-only fallback
    game_batch_size: 24
    train_batch_size: 64
    eval_batch_size: 8

# =============================================================================
# PATHS
# =============================================================================

checkpoint_dir: "./checkpoints"
log_dir: "./logs"

# =============================================================================
# MONITORING
# =============================================================================

prometheus:
  port: 9090
  scrape_interval: 15

grafana:
  port: 3000
