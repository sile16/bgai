# Distributed AlphaZero Training Configuration
# =============================================
#
# ARCHITECTURE (Redis-based, no Ray):
# - Head node: Redis, Coordinator, Training worker, Grafana, Prometheus, MLflow
# - Remote workers: Standalone Python processes connecting via Redis
#
# CONFIG HIERARCHY:
# 1. Global settings (MCTS simulations, max_nodes) - same for all workers
# 2. Device-specific batch sizes - optimized per hardware type
# 3. Local overrides - via CLI flags (--batch-size) for unique machines

# =============================================================================
# GLOBAL TRAINING SETTINGS (applies to ALL workers)
# =============================================================================

mcts:
  simulations: 100        # MCTS iterations per move (GLOBAL)
  max_nodes: 400          # Maximum tree size (GLOBAL)
  temperature: 1.0        # Exploration temperature
  discount: -1.0          # -1 for two-player zero-sum games
  persist_tree: true      # Keep tree between moves

training:
  batch_size: 128         # Samples per training step
  learning_rate: 0.0003   # Adam learning rate
  l2_reg_lambda: 0.0001   # L2 regularization
  games_per_batch: 10     # New games before training step
  steps_per_game: 10      # Training steps per collected game
  surprise_weight: 0.5    # Blend of uniform (0) vs surprise-weighted (1) sampling
  checkpoint_interval: 1000
  max_checkpoints: 5
  # Warm tree: pre-computed deep MCTS tree from initial position
  # Set to 0 to disable. Game workers use this as starting point.
  warm_tree_simulations: 0     # MCTS sims for warm tree (0 = disabled)
  warm_tree_max_nodes: 10000   # Max nodes in warm tree

game:
  max_episode_steps: 500  # Max moves per game
  temperature_start: 1.0  # Initial exploration
  temperature_end: 0.2    # Final exploitation

# =============================================================================
# INFRASTRUCTURE
# =============================================================================

redis:
  host: "100.105.50.111"  # Head node (Tailscale IP)
  port: 6379
  password: "bgai-password"
  buffer_capacity: 100000   # Max experiences
  episode_capacity: 5000    # Max episodes

head:
  host: "100.105.50.111"       # Head node IP (Tailscale)
  host_local: "192.168.20.40"  # LAN fallback

mlflow:
  tracking_uri: "http://100.105.50.111:5000"
  experiment_name: "bgai-training"
  artifact_location: "./mlruns"

coordinator:
  heartbeat_timeout: 30.0   # Seconds before worker considered dead
  heartbeat_interval: 10.0  # Seconds between heartbeats
  weight_push_interval: 10  # Push weights every N training steps

# =============================================================================
# NEURAL NETWORK ARCHITECTURE
# =============================================================================

network:
  hidden_dim: 256
  num_blocks: 6
  num_actions: 156          # Backgammon action space

# =============================================================================
# DEVICE-SPECIFIC BATCH SIZES
# =============================================================================
# These are applied automatically based on detected hardware.
# Only batch sizes vary - MCTS settings remain global.
# Workers can override batch_size via --batch-size CLI flag.

device_configs:
  cuda:
    # NVIDIA GPUs (RTX 4090, etc.)
    game_batch_size: 64
    train_batch_size: 256
    eval_batch_size: 32
    gpu_memory_fraction: 0.25  # Allow multiple workers per GPU

  metal:
    # Apple Silicon (M1, M2, M3)
    game_batch_size: 16
    train_batch_size: 128
    eval_batch_size: 8

  tpu:
    # Google Cloud TPUs
    game_batch_size: 128
    train_batch_size: 512
    eval_batch_size: 64

  cpu:
    # CPU-only fallback
    game_batch_size: 4
    train_batch_size: 64
    eval_batch_size: 4

# =============================================================================
# PATHS
# =============================================================================

checkpoint_dir: "./checkpoints"
log_dir: "./logs"

# =============================================================================
# MONITORING
# =============================================================================

prometheus:
  port: 9090
  scrape_interval: 15

grafana:
  port: 3000
