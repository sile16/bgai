# Distributed AlphaZero Training Configuration
# =============================================
#
# ARCHITECTURE (Redis-based, no Ray):
# - Head node: Redis, Coordinator, Training worker, Grafana, Prometheus, MLflow
# - Remote workers: Standalone Python processes connecting via Redis
#
# CONFIG HIERARCHY:
# 1. Global settings (MCTS simulations, max_nodes) - same for all workers
# 2. Device-specific batch sizes - optimized per hardware type
# 3. Local overrides - via CLI flags (--batch-size) for unique machines

# =============================================================================
# GLOBAL TRAINING SETTINGS (applies to ALL workers)
# =============================================================================


game:
  max_episode_steps: 500  # Max moves per game, safety to prevent infinite moves, todo; log metric: max_episode_limit_hit_count
  short_game: True # starting position is a mid game position rather than complete beginning
  simple_doubles: False # simple doubles only plays doubles to limit randomness

mcts:
  collect_simulations: 100        # MCTS iterations per move (GLOBAL)    changed to collect specifically
  eval_simulations: 50         # added specifically for eval ,
  max_nodes: 400          # Maximum tree size (GLOBAL)
  warm_tree_simulations: 400     # MCTS sims for warm tree (0 = disabled)


  # this is removed, we don't use a static temp, temperature: 0.5        # Exploration temperature                   
  # this is no longer used , we use multi steps per player and this would breka and doesn't make sense. discount: -1.0          # -1 for two-player zero-sum games
  persist_tree: true      # Keep tree between moves
  temperature_start: 0.8  # Initial exploration
  temperature_end: 0.2    # Final exploitation
  # batch_size this should be specific to the hardware, 

training:
  batch_size: 128         # Samples per training step, 
  learning_rate: 0.0003   # Adam learning rate
  bearoff_value_weight: 2.0
  lookup_learning_weight: 1.5         # this is for future if we have specific anchor positions with a confidence value AND policy
  # Bearoff value weight: multiplier for loss on bearoff positions (known-perfect values)
  # Higher values emphasize learning accurate endgame values. Set to 1.0 to disable.
  l2_reg_lambda: 0.0001   # L2 regularization
  games_per_batch: 10     # New games before training step 
  #steps_per_game: 10      # Training steps per collected game Removed .... we should train on all the steps
  surprise_weight: 0.5    # Blend of uniform (0) vs surprise-weighted (1) sampling
  checkpoint_interval: 1000
  max_checkpoints: 5
  # Warm tree: pre-computed deep MCTS tree from initial position
  # Set to 0 to disable. Game workers use this as starting point.


# =============================================================================
# GNU BACKGAMMON EVALUATION SETTINGS
# =============================================================================
# These settings control GNUBG evaluation strength for the gnubg eval type.
# Higher ply and more move filter candidates = stronger but slower evaluation.

gnubg:
  ply: 2                  # Search depth (0=fastest, 1=medium, 2=strongest)
  shortcuts: 0            # Evaluation shortcuts (0=disabled, 1=faster but weaker)
  osdb: 1                 # One-sided bearoff database (0=disabled, 1=enabled)
  move_filters: [8, 4, 2, 2]  # Candidates at each ply level

# =============================================================================
# INFRASTRUCTURE
# =============================================================================

redis:
  host: "100.105.50.111"  # Head node (Tailscale IP)
  host_local: "192.168.20.40"  # LAN fallback
  port: 6379
  password: "bgai-password"
  buffer_capacity: 100000   # Max experiences
  episode_capacity: 5000    # Max episodes

head:
  host: "100.105.50.111"       # Head node IP (Tailscale)
  host_local: "192.168.20.40"  # LAN fallback

mlflow:
  # Use localhost since only the training worker on the head node needs MLFlow
  # This avoids DNS rebinding security issues with external IPs
  tracking_uri: "http://localhost:5000"
  experiment_name: "bgai-training"
  artifact_location: "./mlruns"

coordinator:
  heartbeat_timeout: 30.0   # Seconds before worker considered dead
  heartbeat_interval: 10.0  # Seconds between heartbeats
  weight_push_interval: 10  # Push weights every N training steps

# =============================================================================
# NEURAL NETWORK ARCHITECTURE
# =============================================================================

network:
  hidden_dim: 256
  num_blocks: 6
  num_actions: 156          # Backgammon action space

# =============================================================================
# DEVICE-SPECIFIC BATCH SIZES
# =============================================================================
# These are applied automatically based on detected hardware.
# Only batch sizes vary - MCTS settings remain global.
# Workers can override batch_size via --batch-size CLI flag.

device_configs:
  cuda:
    # NVIDIA GPUs (RTX 4090, etc.)
    game_batch_size: 64
    train_batch_size: 256
    eval_batch_size: 32
    gpu_memory_fraction: 0.25  # Allow multiple workers per GPU

  metal:
    # Apple Silicon (M1, M2, M3)
    game_batch_size: 16
    train_batch_size: 128
    eval_batch_size: 8

  tpu:
    # Google Cloud TPUs
    game_batch_size: 128
    train_batch_size: 512
    eval_batch_size: 64

  cpu:
    # CPU-only fallback
    game_batch_size: 4
    train_batch_size: 64
    eval_batch_size: 4

# =============================================================================
# PATHS
# =============================================================================

checkpoint_dir: "./checkpoints"
log_dir: "./logs"

# =============================================================================
# MONITORING
# =============================================================================

prometheus:
  port: 9090
  scrape_interval: 15

grafana:
  port: 3000
