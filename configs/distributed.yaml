# Distributed AlphaZero Training Configuration
# =============================================
#
# ARCHITECTURE (Redis-based, no Ray):
# - Head node: Redis, Coordinator, Training worker, Grafana, Prometheus, MLflow
# - Remote workers: Standalone Python processes connecting via Redis
#
# CONFIG HIERARCHY:
# 1. Global settings (MCTS simulations, max_nodes) - same for all workers
# 2. Device-specific batch sizes - optimized per hardware type
# 3. Local overrides - via CLI flags (--batch-size) for unique machines

# =============================================================================
# GLOBAL TRAINING SETTINGS (applies to ALL workers)
# =============================================================================


game:
  batch_size: 16               # Base game batch size (overridden by device_configs)
  max_episode_steps: 500  # Max moves per game, safety to prevent infinite moves, todo; log metric: max_episode_limit_hit_count
  short_game: False # switched to full game after 3000+ games collected
  simple_doubles: False # simple doubles only plays doubles to limit randomness

mcts:
  # Temperature schedule (Stochastic MuZero paper style)
  # Values: [1.0, 0.5, 0.1] for first [1e5, 2e5, 3e5] steps, then 0.0 (greedy)
  temperature_schedule_values: [0.5, 0.3, 0.1, 0.0]
  temperature_schedule_steps: [350000, 400000, 500000]
  # Simulation schedule: ramp up sims as temperature decreases
  # At high temp (exploration), fewer sims are needed since moves are random anyway
  # At low temp (greedy), full sims for accurate policy targets
  # Paper used 1600 throughout, but this saves compute early in training
  simulation_schedule_values: [400, 800, 1200, 1600]
  simulation_schedule_steps: [350000, 400000, 500000]
  #mcts settings:
  collect_simulations: 400     # Base MCTS iterations (overridden by simulation_schedule)
  eval_simulations: 50         # MCTS iterations for evaluation
  max_nodes: 4000              # Maximum tree size (must be >= max simulations)
  warm_tree_simulations: 2400   # MCTS sims for warm tree (0 = disabled)
  warm_tree_max_nodes: 4000   # Max nodes for warm tree expansion
  persist_tree: True           # Keep tree between moves


eval:
  eval_types: "random,gnubg"
  #mcts settings
  persist_tree: True
  eval_simulations: 50
  warm_tree_simulations: 500
  max_nodes: 600
  #temp always 0, greedy for eval




training:
  batch_size: 128              # Base training batch size (overridden by device_configs)
  # Continuous training: set very high steps_per_game so training always has backlog
  # Paper used 8M steps / 100K games = 80 steps/game, we use higher to train ahead
  steps_per_epoch: 50000           # Large batch before pushing weights
  learning_rate: 0.0003            # Adam learning rate
  decode_threads: 8                # CPU threads for msgpack decode/stack (0=off)
  batch_reuse_steps: 4             # Run N updates per sampled batch to reduce CPU bottleneck
  # When training and GPU collection share the same GPU (head node), pause GPU collection
  # during training to avoid severe contention and keep training throughput stable.
  pause_collection_during_training: true
  # Bearoff value weight: multiplier for loss on bearoff positions (known-perfect values)
  # Higher values emphasize learning accurate endgame values. Set to 1.0 to disable.
  bearoff_enabled: True           # disable bearoff DB completely
  bearoff_table_path: "./data/bearoff_15_v4"  # path to bearoff table (without extension, adds .bin and .json)
  bearoff_value_weight: 2.0        # the learning rate multiplier for values pulled from bearoff db
  lookup_enabled: False            # disables the lookup positions.
  lookup_learning_weight: 1.5      # this is for future if we have specific anchor positions with value AND policy
  l2_reg_lambda: 0.0001            # L2 regularization
  # Continuous training: 1 game = 1000 steps target means training always runs
  # (we have 5K+ games = 5M+ target steps, way ahead of actual ~375K steps)
  games_per_epoch: 1               # Trigger training after every game (continuous)
  steps_per_game: 1000             # High ratio ensures training always has backlog
  surprise_weight: 0.0             # Uniform sampling (paper used uniform for backgammon)
  checkpoint_epoch_interval: 10    # Checkpoint every 10 epochs (500K steps)
  max_checkpoints: 10              # Keep more checkpoints

# =============================================================================
# GNU BACKGAMMON EVALUATION SETTINGS
# =============================================================================
# These settings control GNUBG evaluation strength for the gnubg eval type.
# Higher ply and more move filter candidates = stronger but slower evaluation.

gnubg:
  ply: 1                  # Search depth (0=fastest, 1=medium, 2=strongest)
  shortcuts: 0            # Evaluation shortcuts (0=disabled, 1=faster but weaker)
  osdb: 1                 # One-sided bearoff database (0=disabled, 1=enabled)
  #  what is this? move_filters: [8, 4, 2, 2]  # Candidates at each ply level

# =============================================================================
# INFRASTRUCTURE
# =============================================================================

redis:
  host: "localhost"
  db: 0
  # use head host host: "100.105.50.111"  # Head node (Tailscale IP)
  # use head host_local host_local: "192.168.20.40"  # LAN fallback
  port: 6379
  password: "bgai-password"
  buffer_capacity: 10000000  # Max experiences (~10M, matches paper's 100K games * ~100 moves)
  episode_capacity: 100000   # Max episodes/games (matches Stochastic MuZero paper)

head:
  host: "100.105.50.111"       # Head node IP (Tailscale)
  host_local: "192.168.20.40"  # LAN fallback

mlflow:
  # Use localhost since only the training worker on the head node needs MLFlow
  # This avoids DNS rebinding security issues with external IPs
  # removed, use head host or head host_localtracking_uri: "http://localhost:5000"
  port: 5000
  experiment_name: "bgai-training"
  artifact_location: "./mlruns"

coordinator:
  heartbeat_timeout: 30.0   # Seconds before worker considered dead
  heartbeat_interval: 10.0  # Seconds between heartbeats
  weight_push_interval: 10  # Push weights every N training steps

# =============================================================================
# NEURAL NETWORK ARCHITECTURE
# =============================================================================

network:
  hidden_dim: 256
  num_blocks: 6
  num_actions: 156          # Backgammon action space
  # does this incorporate the new output value dimension size? 


# =============================================================================
# DEVICE-SPECIFIC BATCH SIZES
# =============================================================================
# These are applied automatically based on detected hardware.
# Only batch sizes vary - MCTS settings remain global.
# Workers can override batch_size via --batch-size CLI flag.

device_configs:
  cuda:
    # NVIDIA GPUs (RTX 4090, etc.)
    # With mutually exclusive training/collection, each gets full GPU compute
    # Memory is split 45%/45% but compute alternates for 100% utilization
    # batch_size=128 with max_nodes=2000 fits in 11GB with ~62% GPU util
    # Keep this conservative so the head GPU game worker stays alive.
    game_batch_size: 128
    # Fixed per-process GPU memory caps (GB). Prefer these over fractions.
    # Keep sums below total VRAM so multiple JAX processes can coexist.
    game_gpu_memory_gb: 17.0
    train_batch_size: 4096
    train_gpu_memory_gb: 6.0
    eval_batch_size: 32
    eval_gpu_memory_gb: 2.0

  metal:
    # Apple Silicon (M1, M2, M3)
    game_batch_size: 24
    train_batch_size: 128
    eval_batch_size: 16

  tpu:
    # Google Cloud TPUs
    game_batch_size: 128
    train_batch_size: 512
    eval_batch_size: 64

  cpu:
    # CPU-only fallback
    game_batch_size: 24
    train_batch_size: 64
    eval_batch_size: 8

# =============================================================================
# PATHS
# =============================================================================

checkpoint_dir: "./checkpoints"
log_dir: "./logs"

# =============================================================================
# MONITORING
# =============================================================================

prometheus:
  port: 9090
  scrape_interval: 15

grafana:
  port: 3000
